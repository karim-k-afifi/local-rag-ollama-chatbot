# Configuration for the Local RAG Chatbot - Example

# Ollama Settings
# Ensure this points to your running Ollama instance's OpenAI-compatible endpoint
OLLAMA_URL=http://localhost:11434/v1

# The Ollama model to use (make sure you have pulled it: ollama pull <model_name>)
OLLAMA_MODEL=llama3.2:latest

# API Key for Ollama (required by the OpenAI client, but the value doesn't matter for local Ollama)
OLLAMA_API_KEY=ollama 